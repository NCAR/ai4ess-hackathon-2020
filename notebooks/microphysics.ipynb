{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AI for Earth System Science Hackathon 2020\n",
    "# Microphysics Machine Learning Challenge Problem\n",
    "\n",
    "Andrew Gettelman, Jack Chen, David John Gagne\n",
    "\n",
    "## Introduction\n",
    "Cloud processes are perhaps the most critical and uncertain processes for weather and climate prediction. The complex nature of sub grid scale clouds makes traceable simulation of clouds across scales difficult (or impossible). There exist many observations and detailed simulations of clouds that are used to develop and evaluate larger scale models. Many times these models and measurements are used to develop empirical relationships for large scale models to be computationally efficient. Machine learning provides another potential tool to improve our empirical parameterizations of clouds. Here we present a comprehensive investigation of replacing the warm rain formation process in an earth system model with emulators that use detailed treatments from small scale and idealized models to represent key cloud microphysical processes. \n",
    "\n",
    "The warm rain formation process is critical for weather and climate prediction. When rain forms governs the location, intensity and duration of rainfall events, critical for weather and the hydrologic cycle. Rain formation also affects cloud lifetime and the radiative properties of low clouds, making it critical for predicting climate (twomey1977,albrecht1989) The specific process of rain formation is altered by the microphysical properties of clouds, making rain formation susceptible to the size distribution of cloud drops, and ultimately to the distribution of aerosol particles that act as Cloud Condensation Nuclei. \n",
    "\n",
    "Ice of course will complicate the precipitation process. Supercooled liquid drops can exist, and these will either precipitation in a similar manner to warm precipitation (with no ice involved) and subsequently may freeze once they are rain drops. Or cloud droplets may freeze and form ice crystals, which precipitate and collect liquid, freezing or riming as they fall. We will not concern ourselves in this work with processes involving (or potentially involving) ice. This of course is a critical issue for weather (forbes2014)and climate (gettelman2019b,bodas-salcedo2019)prediction. \n",
    "\n",
    "The representation of rain formation in clouds involves the interaction of a population of hydrometeors. For warm clouds, the process is one of collision and coalescence, usually defined with a detailed process of stochastic collection (pruppacher1997). The stochastic collection process describes how each size particle interacts with other sizes. Usually there is a distribution of small cloud drops with an extension or separate distribution of rain drops whose interactions are evaluated. \n",
    "\n",
    "The stochastic collection process is computationally expensive to treat directly in large scale global models for weather and climate prediction. It requires the pre-computation of a collection kernel for how different sizes of hydrometeors will interact due to differential fall speeds, and it requires tracking populations discretized by bins. This tracking and advection of the order of 60 different bins for liquid and ice combined makes it computationally expensive. So traditionally, large scale models with bulk microphysics treat the stochastic collection process of warm rain formation in a heavily parameterized fashion (khairoutdinov2000,seifert200) For conceptual simplicity, the process is often broken up into two processes. Autoconversion is the transition of cloud drops into rain as part of a cloud droplet distribution grows to large sizes. Methods for determining autoconversion and accretion are varied. Because they are the major loss mechanism for cloud water different descriptions of the processes result in very different model evolution and climates (michibata2015).\n",
    "\n",
    "Because many methods for autoconversion and accretion are just empirical fits to data or other models, they are readily applicable to replacement with more sophisticated tools. Neural Networks are multivariate emulators that allow many more degrees of freedom than traditional polynomial methods for example.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Software Requirements\n",
    "This notebook requires Python >= 3.7. The following libraries are required:\n",
    "* numpy\n",
    "* scipy\n",
    "* pandas\n",
    "* matplotlib\n",
    "* xarray\n",
    "* scikit-learn\n",
    "* tensorflow >= 2.1\n",
    "* netcdf4\n",
    "* h5netcdf\n",
    "* tqdm\n",
    "* pyyaml\n",
    "* s3fs\n",
    "* pyarrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install numpy scipy pandas matplotlib xarray scikit-learn tensorflow netcdf4 h5netcdf tqdm pyyaml s3fs pyarrow mlmicrophysics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install --upgrade pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if working on google colab\n",
    "! pip install -U -q PyDrive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "The Community Atmosphere Model version 6 (CAM6) is the atmospheric component of the Community Earth System Model version 2 (danabasoglu2020). CAM6 features a two-moment stratiform cloud microphysics scheme [hereafter MG2](gettelman2015b,gettelman2015a) with prognostic liquid, ice, rain and snow hydrometeor classes. MG2 permits ice supersaturation. CAM6 includes a physically based ice mixed phase dust ice nucleation scheme (hoose2010) with modifications for a distribution of contact angles (wang2014), and accounts for preexisting ice in the cirrus ice nucleation of (liu2005) as described by (shi2015).\n",
    "\n",
    "MG2 is coupled to a unified moist turbulence scheme, Cloud Layers Unified by Binormals (CLUBB), developed by (golaz2002) and (larson2002) and implemented in CAM by (bogenschutz2013). CLUBB handles stratiform clouds, boundary layer moist turbulence and shallow convective motions. CAM6 also has an ensemble plume mass flux deep convection scheme described by (zhang1995) and (neale2008), which has very simple microphysics. The radiation scheme is The Rapid Radiative Transfer Model for General Circulation Models (RRTMG) (iacono2000).\n",
    "\n",
    "Within the MG2 parameterization, the warm rain formation process is represented by equations for autoconversion and accretion from (khairoutdinov2000), hereafter KK2000. KK2000 uses empirical fits to a large eddy simulation with bin-resolved microphysics to define:\n",
    "\\begin{equation}\n",
    "    \\left(\\frac{\\partial q_r}{\\partial t} \\right)_{AUTO} = 13.5 q_c^{2.47} N_c^{-1.1}\n",
    "\\end{equation}\n",
    "\\begin{equation}\n",
    "    \\left(\\frac{\\partial q_r}{\\partial t} \\right)_{ACCRE} = 67 (q_c q_r)^{1.15}\n",
    "\\end{equation}\n",
    "Where $q_c$ and $q_r$ are mass mixing ratios for condensate and rain, and $N_c$ is the number concentration of condensate. For CAM6 the autconversion rate exponent and prefactor has been adjusted from the original (khairoutdinov2000) scheme to better match observations (gettelman2019b).\n",
    "\n",
    "#### Stochastic Collection\n",
    "\n",
    "We replace the KK2000 process rate equations with an estimate of the stochastic collection process from the Tel Aviv University (TAU) model. The TAU model uses a \"bin\" or \"sectional\" approach, where the drop size distribution is resolved into 35 size bins. It differs from most other microphysical codes in that it solves for two  moments of the drop size distribution in each of the bins. This allows for a more accurate transfer of mass between bins and alleviates anomalous drop growth. The original components were developed by Tzivion et al. (1987), (1989), Feingold et al. (1988) with later applications and development documented in Reisin et al. (1996), Stevens et al. (1996), Feingold et al. (1999), Tzivion et al. (1999), Yin et al (2000) and Harrington et al. (2000). \n",
    "\n",
    "Cloud Parcel Model Documentation here: https://www.esrl.noaa.gov/csl/staff/graham.feingold/code/readme.html\n",
    "\n",
    "First we convert the size distributions for liquid and rain into number concentrations in individual size bins. Liquid and rain are put in the same continuous distribution of 32 size bins for the TAU code. Then we use this as input to the TAU code, running the stochastic collection kernel. The result is a revised set of 32 bins with number concentration in each bin. We the find a minimum in the distribution if present: this is always found in the case where there is rain and condensate present at the end of the calculation. The minimum is typically between 40 and 100 microns (diameter). This minimium is used to divide the bins into liquid and rain. The total number and mass in each is defined, and tendencies calculated as the final mass and number minus the initial mass and number divided by the timestep. A limiter is applied to ensure that the mass and number are non-zero, and tendencies limited to ensure this. This estimated stochastic collection tendency is then applied instead of the accretion and autoconversion tendencies.\n",
    "\n",
    "The code does run the accretion and autoconversion from MG2 on the same state, and we can save this off as a diagnostic, so we can directly compare the original MG2 tendency (autoconversion + accretion) with the stochastic collection tendency from the TAU code. \n",
    "\n",
    "The microphysics datasets contains 176 files containing \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time span of the dataset\n",
    "|  | Datetime |\n",
    "| ---- | :----:|\n",
    "| Start    | Jan 1     |\n",
    "| Length   | 2 years   |\n",
    "\n",
    "### Geographic Coverage of Dataset\n",
    "|  | Latitude | Longitude |\n",
    "| ------------- | :----:|:----------- |\n",
    "| Max      | 90     | 358.75 |\n",
    "| Min      | -90    | 0 |\n",
    "\n",
    "### Potential Input Variables\n",
    "| Variable Name | Units | Description |\n",
    "| ------------- | :----:|:----------- |\n",
    "| QC_TAU_in      | kg/kg     | cloud water mixing ratio |\n",
    "| NC_TAU_in      | kg<sup>-1</sup>     | cloud droplet column concentration |\n",
    "| QR_TAU_in      | kg/kg     | rain water mixing ratio |\n",
    "| NR_TAU_in      | kg<sup>-1</sup>     | rain droplet column concentration |\n",
    "| RHO_CLUBB_lev  | kg/m<sup>3</sup>     | air density at center of grid cell |\n",
    "\n",
    "### Output Variables\n",
    "| Variable Name | Units | Description |\n",
    "| ------------- | :----:|:----------- |\n",
    "| qrtend_TAU      | kg/kg/s     | qr tendency due to autoconversion & accretion in TAU bin |\n",
    "| nrtend_TAU      | kg/kg/s     | nr tendency due to autoconversion & accretion in TAU bin |\n",
    "| nctend_TAU      | kg/kg/s     | nc tendency due to autoconversion & accretion in TAU bin |\n",
    "\n",
    "### Meta Variables\n",
    "| Variable Name | Units | Description |\n",
    "| ------------- | :----:|:----------- |\n",
    "| lat      | degrees_north     | latitude |\n",
    "| lev      | hPa     | atmospheric level |\n",
    "| lon      | degrees_east     | longitude |\n",
    "| depth      | arbitrary     | depth index |\n",
    "| row      | arbitrary     | row index |\n",
    "| col      | arbitrary     | column index |\n",
    "| pressure      | Pa     | atmospheric pressure |\n",
    "| temperature      | K     | temperature derived from pressure and density |\n",
    "| time      | days     | time in days |\n",
    "| qrtend_MG2      | kg/kg/s     | qr tendency due to autoconversion & accretion in MG2 |\n",
    "| nrtend_MG2      | kg/kg/s     | nr tendency due to autoconversion & accretion in MG2 |\n",
    "| nctend_MG2      | kg/kg/s     | nc tendency due to autoconversion & accretion in MG2 |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training, Validation, and Test Datasets\n",
    "\n",
    "There are 176 files that will be split into training, validation, and test datsets via indices found in the `subset_data` variable defined below. In total, these files contain 85,263,948 data points and is randomly sampled using the `subsample` variable below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import random\n",
    "import sys\n",
    "import yaml\n",
    "from os.path import join, exists\n",
    "import os\n",
    "import s3fs\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, MaxAbsScaler, RobustScaler\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, mean_absolute_error\n",
    "import tensorflow as tf\n",
    "\n",
    "from mlmicrophysics.metrics import heidke_skill_score, peirce_skill_score, hellinger_distance, root_mean_squared_error, r2_corr\n",
    "from mlmicrophysics.models import DenseNeuralNetwork\n",
    "from mlmicrophysics.data import subset_data_files_by_date, assemble_data_files\n",
    "\n",
    "\n",
    "# set random seed\n",
    "seed = 328942\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "tf.random.set_seed(seed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# define data parameters\n",
    "\n",
    "data_path = \"ncar-aiml-data-commons/microphysics\"\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "if IN_COLAB:\n",
    "    out_path = \"/content/gdrive/My Drive/micro_models/base\"\n",
    "else:\n",
    "    out_path = \"./micro_models/base/\"\n",
    "if not exists(out_path):\n",
    "    os.makedirs(out_path)\n",
    "\n",
    "subsample = 0.1\n",
    "input_cols = [\"QC_TAU_in\", \"NC_TAU_in\", \"QR_TAU_in\", \"NR_TAU_in\", \"RHO_CLUBB_lev\"]\n",
    "output_cols = [\"qrtend_TAU\", \"nctend_TAU\", \"nrtend_TAU\"]\n",
    "subset_data = {\"train_date_start\" : 0,\n",
    "               \"train_date_end\" : 11000,\n",
    "               \"test_date_start\" : 11100,\n",
    "               \"test_date_end\" : 17500}\n",
    "\n",
    "input_scaler = StandardScaler()\n",
    "input_transforms = {\"QC_TAU_in\" : \"log10_transform\",\n",
    "                    \"NC_TAU_in\" : \"log10_transform\",\n",
    "                    \"QR_TAU_in\" : \"log10_transform\",\n",
    "                    \"NR_TAU_in\" : \"log10_transform\"}\n",
    "\n",
    "output_transforms = {\"qrtend_TAU\" : {0: [\"<=\", 1e-18, \"zero_transform\", \"None\"],\n",
    "                                   1: [\">\", 1e-18, \"log10_transform\", \"StandardScaler\"]},\n",
    "                     \"nctend_TAU\" : {0: [\">=\", -1e-18, \"zero_transform\", \"None\"],\n",
    "                                   1: [\"<\", -1e-18, \"neg_log10_transform\", \"StandardScaler\"]},\n",
    "                     \"nrtend_TAU\" : {-1: [\"<\", 0, \"neg_log10_transform\", \"StandardScaler\"],\n",
    "                                   0: [\"==\", 0, \"zero_transform\", \"None\"],\n",
    "                                   1: [\">\", 0, \"log10_transform\", \"StandardScaler\"]}}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data from disk or cloud\n",
    "# Separate input, output and meta data\n",
    "# Split into training, validation, and test sets\n",
    "\n",
    "print(\"Subsetting file paths by train, validation, and test\")\n",
    "train_files, val_files, test_files = subset_data_files_by_date(data_path, **subset_data)\n",
    "\n",
    "print(\"\\nLoading training data\")\n",
    "scaled_input_train, \\\n",
    "labels_train, \\\n",
    "transformed_out_train, \\\n",
    "scaled_out_train, \\\n",
    "output_scalers, \\\n",
    "meta_train = assemble_data_files(train_files, input_cols, output_cols, input_transforms,\n",
    "                                 output_transforms, input_scaler, subsample=subsample)\n",
    "\n",
    "print(\"\\nLoading testing data\")\n",
    "scaled_input_test, \\\n",
    "labels_test, \\\n",
    "transformed_out_test, \\\n",
    "scaled_out_test, \\\n",
    "output_scalers_test, \\\n",
    "meta_test = assemble_data_files(test_files, input_cols, output_cols, input_transforms,\n",
    "                                output_transforms, input_scaler, output_scalers=output_scalers,\n",
    "                                train=False, subsample=subsample)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save meta data, input scalers, and output scalers\n",
    "    \n",
    "meta_test.to_csv(join(out_path, \"meta_test.csv\"), index_label=\"index\")\n",
    "\n",
    "input_scaler_df = pd.DataFrame({\"mean\": input_scaler.mean_, \"scale\": input_scaler.scale_},\n",
    "                               index=input_cols)\n",
    "input_scaler_df.to_csv(join(out_path, \"input_scale_values.csv\"), index_label=\"input\")\n",
    "\n",
    "\n",
    "# you will need to match your scaler attributes saved in this code section to your chosen output scalers\n",
    "out_scales_list = []\n",
    "for var in output_scalers.keys():\n",
    "    for out_class in output_scalers[var].keys():\n",
    "        if output_scalers[var][out_class] is not None:\n",
    "            out_scales_list.append(pd.DataFrame({\"mean\": output_scalers[var][out_class].mean_,\n",
    "                                                 \"var\": output_scalers[var][out_class].var_,\n",
    "                                                 \"scale\": output_scalers[var][out_class].scale_},\n",
    "                                                index=[var + \"_\" + str(out_class)]))\n",
    "out_scales_df = pd.concat(out_scales_list)\n",
    "out_scales_df.to_csv(join(out_path, \"output_scale_values.csv\"),\n",
    "                     index_label=\"output\")\n",
    "out_scales_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histograms of original training input data by column\n",
    "\n",
    "fig, axes = plt.subplots(1,5, figsize=(20, 3))\n",
    "transformed_input_train = pd.DataFrame(input_scaler.inverse_transform(scaled_input_train), columns=input_cols)\n",
    "for a, ax in enumerate(axes.ravel()):\n",
    "    if a < len(input_cols):\n",
    "        ax.set_yscale(\"log\")\n",
    "        ax.hist(transformed_input_train[input_cols[a]], bins=20)\n",
    "        ax.set_title(input_cols[a])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inverse transform and scaling of scaled train data\n",
    "\n",
    "output_col = \"nrtend_TAU\"\n",
    "original_out_train_nr = np.zeros(scaled_out_train.shape[0])\n",
    "original_out_train_nr[labels_train[output_col] == 1] = 10 ** output_scalers[output_col][1].inverse_transform(\n",
    "    scaled_out_train.loc[labels_train[output_col] == 1, [output_col]]).ravel()\n",
    "original_out_train_nr[labels_train[output_col] == -1] = -10 ** output_scalers[output_col][-1].inverse_transform(\n",
    "    scaled_out_train.loc[labels_train[output_col] == -1, [output_col]]).ravel()\n",
    "\n",
    "output_col = \"nctend_TAU\"\n",
    "original_out_train_nc = np.zeros(scaled_out_train.shape[0])\n",
    "original_out_train_nc[labels_train[output_col] == 1] = -10 ** output_scalers[output_col][1].inverse_transform(\n",
    "    scaled_out_train.loc[labels_train[output_col] == 1, [output_col]]).ravel()\n",
    "\n",
    "output_col = \"qrtend_TAU\"\n",
    "original_out_train_qr = np.zeros(scaled_out_train.shape[0])\n",
    "original_out_train_qr[labels_train[output_col] == 1] = 10 ** output_scalers[output_col][1].inverse_transform(\n",
    "    scaled_out_train.loc[labels_train[output_col] == 1, [output_col]]).ravel()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output visualizations\n",
    "\n",
    "f, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(16,4))\n",
    "\n",
    "output_col = \"nrtend_TAU\"\n",
    "ax1.hist(np.log10(-original_out_train_nr[labels_train[output_col] == -1]), bins=50, label=\"label == -1\")\n",
    "ax1.hist(np.log10(original_out_train_nr[labels_train[output_col] == 1]), bins=50, label=\"label == 1\")\n",
    "ax1.set_xlabel(output_col)\n",
    "ax1.set_ylabel('log10')\n",
    "ax1.title.set_text(f\"log10-transformed {output_col} output data\\nfiltered by output_transform ops\")\n",
    "ax1.set_yscale('log')\n",
    "ax1.legend()\n",
    "\n",
    "output_col = \"nctend_TAU\"\n",
    "ax2.hist(np.log10(-original_out_train_nc[labels_train[output_col] == 1]), bins=50, label=\"label == 1\")\n",
    "ax2.set_xlabel(output_col)\n",
    "ax2.set_ylabel('log10')\n",
    "ax2.title.set_text(f\"log10-transformed {output_col} output data\\nfiltered by output_transform ops\")\n",
    "ax2.set_yscale('log')\n",
    "ax2.legend()\n",
    "\n",
    "output_col = \"qrtend_TAU\"\n",
    "ax3.hist(np.log10(original_out_train_qr[labels_train[output_col] == 1]), bins=50, label=\"label == 1\")\n",
    "ax3.set_xlabel(output_col)\n",
    "ax3.set_ylabel('log10')\n",
    "ax3.title.set_text(f\"log10-transformed {output_col} output data\\nfiltered by output_transform ops\")\n",
    "ax3.set_yscale('log')\n",
    "ax3.legend()\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load and view a single file\n",
    "\n",
    "fs = s3fs.S3FileSystem(anon=True)\n",
    "filenames = fs.ls(\"s3://ncar-aiml-data-commons/microphysics\")\n",
    "fobj = fs.open(filenames[0])\n",
    "single_file = pd.read_parquet(fobj).set_index('Index')\n",
    "single_file.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline Machine Learning Model\n",
    "Description of baseline ML approach should include:\n",
    "* Choice of ML software\n",
    "* Type of ML model\n",
    "* Hyperparameter choices and justification\n",
    "\n",
    "A baseline model for solving this problem uses an in-series classifier to regressor neural network architecture implemented in Keras.  Initially, there are three classifier networks that feed into four regressor networks. Each classifier and regressor network has 4 hidden layers of 30 neurons each and relu activation. Those hidden layers then feed into a final output layer of size 2 or 3 for classification (1 and 0 or 1, 0, and -1) and of size 1 for regression. The classifier models are trained using the categorial crosstenropy loss function while the regression models are trained using the mean squared error loss function.\n",
    "\n",
    "<center><img src='micro_images/mlmicrophysics_nn.png'><center>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model hyper parameters\n",
    "\n",
    "classifier_metrics = [\"acc\", \"pss\", \"hss\"]\n",
    "regressor_metrics = [\"mse\", \"mae\", \"r2\", \"hellinger\"]\n",
    "\n",
    "classifier_networks = {\"hidden_layers\" : 4,\n",
    "                       \"hidden_neurons\" : 30,\n",
    "                       \"loss\" : \"categorical_crossentropy\",\n",
    "                       \"output_activation\" : \"softmax\",\n",
    "                       \"activation\" : \"relu\",\n",
    "                       \"epochs\" : 15,\n",
    "                       \"batch_size\" : 1024,\n",
    "                       \"verbose\" : 1,\n",
    "                       \"lr\" : 0.0001,\n",
    "                       \"l2_weight\" : 1.0e-5,\n",
    "                       \"classifier\" : 1}\n",
    "\n",
    "regressor_networks = {\"hidden_layers\" : 4,\n",
    "                      \"hidden_neurons\" : 30,\n",
    "                      \"loss\" : \"mse\",\n",
    "                      \"output_activation\" : \"linear\",\n",
    "                      \"activation\" : \"relu\",\n",
    "                      \"epochs\" : 15,\n",
    "                      \"batch_size\" : 1024,\n",
    "                      \"verbose\" : 1,\n",
    "                      \"lr\" : 0.0001,\n",
    "                      \"l2_weight\" : 1.0e-5,\n",
    "                      \"classifier\" : 0}\n",
    "\n",
    "# hyperparameter dictionaries\n",
    "class_metrics = {\"accuracy\": accuracy_score,\n",
    "                 \"heidke\": heidke_skill_score,\n",
    "                 \"peirce\": peirce_skill_score}\n",
    "\n",
    "reg_metrics = {\"rmse\": root_mean_squared_error,\n",
    "               \"mae\": mean_absolute_error,\n",
    "               \"r2\": r2_corr,\n",
    "               \"hellinger\": hellinger_distance}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build and fit the model\n",
    "\n",
    "histories = {\"classifiers\": {}, \"regressors\": {}}\n",
    "classifiers = dict()\n",
    "regressors = dict()\n",
    "reg_index = []\n",
    "for output_col in output_cols:\n",
    "    for label in list(output_transforms[output_col].keys()):\n",
    "        if label != 0:\n",
    "            reg_index.append(output_col + f\"_{label:d}\")\n",
    "test_prediction_values = np.zeros((scaled_out_test.shape[0], len(reg_index)))\n",
    "test_prediction_labels = np.zeros(scaled_out_test.shape)\n",
    "classifier_scores = pd.DataFrame(0, index=output_cols, columns=[\"accuracy\", \"heidke\", \"peirce\"])\n",
    "confusion_matrices = dict()\n",
    "reg_cols = [\"rmse\", \"mae\", \"r2\", \"hellinger\"]\n",
    "reg_scores = pd.DataFrame(0, index=reg_index, columns=reg_cols)\n",
    "l = 0\n",
    "\n",
    "for o, output_col in enumerate(output_cols):\n",
    "    print(\"Train Classifer \", output_col)\n",
    "    classifiers[output_col] = DenseNeuralNetwork(**classifier_networks)\n",
    "    hist = classifiers[output_col].fit(scaled_input_train,\n",
    "                                       labels_train[output_col],\n",
    "                                       scaled_input_test,\n",
    "                                       labels_test[output_col])\n",
    "    histories[\"classifiers\"][output_col] = hist\n",
    "    classifiers[output_col].save_fortran_model(join(out_path,\n",
    "                                                    \"dnn_{0}_class_fortran.nc\".format(output_col[0:2])))\n",
    "    classifiers[output_col].model.save(join(out_path,\"dnn_{0}_class.h5\".format(output_col[0:2])))\n",
    "    regressors[output_col] = dict()\n",
    "    histories[\"regressors\"][output_col] = dict()\n",
    "    print(\"Evaluate Classifier\", output_col)\n",
    "    test_prediction_labels[:, o] = classifiers[output_col].predict(scaled_input_test)\n",
    "    confusion_matrices[output_col] = confusion_matrix(labels_test[output_col],\n",
    "                                                      test_prediction_labels  [:, o])\n",
    "    for class_score in classifier_scores.columns:\n",
    "        classifier_scores.loc[output_col, class_score] = class_metrics[class_score](labels_test[output_col],\n",
    "                                                                                    test_prediction_labels[:, o])\n",
    "    print(classifier_scores.loc[output_col])\n",
    "    for label in list(output_transforms[output_col].keys()):\n",
    "        if label != 0:\n",
    "            print(\"Train Regressor \", output_col, label)\n",
    "            regressors[output_col][label] = DenseNeuralNetwork(**regressor_networks)\n",
    "            hist = regressors[output_col][label].fit(scaled_input_train.loc[labels_train[output_col] == label],\n",
    "                                                     scaled_out_train.loc[labels_train[output_col] == label, output_col],\n",
    "                                                     scaled_input_test.loc[labels_test[output_col] == label],\n",
    "                                                     scaled_out_test.loc[labels_test[output_col] == label, output_col])\n",
    "            histories[\"regressors\"][output_col][label] = hist\n",
    "\n",
    "            if label > 0:\n",
    "                out_label = \"pos\"\n",
    "            else:\n",
    "                out_label = \"neg\"\n",
    "            regressors[output_col][label].save_fortran_model(join(out_path,\n",
    "                                                                  \"dnn_{0}_{1}_fortran.nc\".format(output_col[0:2],\n",
    "                                                                                                  out_label)))\n",
    "            regressors[output_col][label].model.save(join(out_path,\n",
    "                                                          \"dnn_{0}_{1}.h5\".format(output_col[0:2], out_label)))\n",
    "            print(\"Test Regressor\", output_col, label)\n",
    "            test_prediction_values[:, l] = output_scalers[output_col][label].inverse_transform(regressors[output_col][label].predict(scaled_input_test).reshape(-1,1)).flatten()\n",
    "            reg_label = output_col + f\"_{label:d}\"\n",
    "            for reg_col in reg_cols:\n",
    "                reg_scores.loc[reg_label,\n",
    "                               reg_col] = reg_metrics[reg_col](transformed_out_test.loc[labels_test[output_col] == label,\n",
    "                                                                                        output_col],\n",
    "                                                                test_prediction_values[labels_test[output_col] == label, l])\n",
    "            print(reg_scores.loc[reg_label])\n",
    "            l += 1\n",
    "print(\"Saving data\")\n",
    "classifier_scores.to_csv(join(out_path, \"dnn_classifier_scores.csv\"), index_label=\"Output\")\n",
    "reg_scores.to_csv(join(out_path, \"dnn_regressor_scores.csv\"), index_label=\"Output\")\n",
    "test_pred_values_df = pd.DataFrame(test_prediction_values, columns=reg_index)\n",
    "test_pred_labels_df = pd.DataFrame(test_prediction_labels, columns=output_cols)\n",
    "test_pred_values_df.to_csv(join(out_path, \"test_prediction_values.csv\"), index_label=\"index\")\n",
    "test_pred_labels_df.to_csv(join(out_path, \"test_prediction_labels.csv\"), index_label=\"index\")\n",
    "labels_test.to_csv(join(out_path, \"test_cam_labels.csv\"), index_label=\"index\")\n",
    "transformed_out_test.to_csv(join(out_path, \"test_cam_values.csv\"), index_label=\"index\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize classifier model performance\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "for k in histories['classifiers'].keys():\n",
    "    plt.plot(histories['classifiers'][k]['loss'], label=f\"{k} loss\")\n",
    "    plt.plot(histories['classifiers'][k]['val_loss'], label=f\"{k} val_loss\")\n",
    "plt.title('Classifier model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize regressor model performance\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "for k in histories['regressors'].keys():\n",
    "    for l in histories['regressors'][k].keys():\n",
    "        plt.plot(histories['regressors'][k][l]['loss'], label=f\"{k} label {l} loss\")\n",
    "        plt.plot(histories['regressors'][k][l]['val_loss'], label=f\"{k} label {l} val_loss\")\n",
    "plt.title('regressor model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics\n",
    "\n",
    "Prediction metrics by output variable for classifier networks:\n",
    "\n",
    "| Variable Name | accuracy | heidke | peirce |\n",
    "| ------------- |:----------- |:----------- |:----------- |\n",
    "| qrtend_TAU  |  0.98     | 0.97 | 0.99 | \n",
    "| nctend_TAU  |  0.99     | 0.99 | 0.97 |\n",
    "| nrtend_TAU  |  0.98     | 0.97 | 0.99 |\n",
    "\n",
    "Prediction metrics by output variable for regression networks:\n",
    "\n",
    "| Variable Name | rmse | mae | r2 | hellinger |\n",
    "| ------------- |:----------- |:----------- |:----------- |:----------- |\n",
    "| qrtend_TAU_1  |  0.20     | 0.10 | 0.99 | 0.00056 |\n",
    "| nctend_TAU_1  |  0.17     | 0.08 | 1.00 | 0.00099 |\n",
    "| nrtend_TAU_-1  |  0.20     | 0.11 | 0.99 | 0.00056 |\n",
    "| nrtend_TAU_1 | 0.25 | 0.16 | 0.98 | 0.00018 |\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unscaled predicted output values\n",
    "\n",
    "pred_tendencies = pd.DataFrame(0, index=scaled_out_test.index, columns=output_cols, dtype=float)\n",
    "\n",
    "nr_pred_values = np.zeros(scaled_input_test.shape[0])\n",
    "nr_pred_values[test_pred_labels_df[\"nrtend_TAU\"] == 1] = (10 ** test_pred_values_df.loc[test_pred_labels_df[\"nrtend_TAU\"] == 1, [\"nrtend_TAU_1\"]]).values.flatten()\n",
    "nr_pred_values[test_pred_labels_df[\"nrtend_TAU\"] == -1] = (-10 ** test_pred_values_df.loc[test_pred_labels_df[\"nrtend_TAU\"] == -1, [\"nrtend_TAU_-1\"]]).values.flatten()\n",
    "pred_tendencies.loc[:, \"nrtend_TAU\"] = nr_pred_values\n",
    "\n",
    "pred_tendencies.loc[test_pred_labels_df[\"nctend_TAU\"] == 1, \"nctend_TAU\"] = (-10 ** test_pred_values_df.loc[test_pred_labels_df[\"nctend_TAU\"] == 1, [\"nctend_TAU_1\"]]).values.flatten()\n",
    "\n",
    "pred_tendencies.loc[test_pred_labels_df[\"qrtend_TAU\"] == 1, \"qrtend_TAU\"] = (10 ** test_pred_values_df.loc[test_pred_labels_df[\"qrtend_TAU\"] == 1, [\"qrtend_TAU_1\"]]).values.flatten()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unscaled actual output values\n",
    "\n",
    "unscaled_tendencies = pd.DataFrame(0, index=scaled_out_test.index, columns=output_cols, dtype=float)\n",
    "\n",
    "nr_values = np.zeros(scaled_input_test.shape[0])\n",
    "nr_values[labels_test[\"nrtend_TAU\"] == 1] = (10 ** output_scalers[\"nrtend_TAU\"][1].inverse_transform(\n",
    "    scaled_out_test.loc[labels_test[\"nrtend_TAU\"] == 1, [\"nrtend_TAU\"]])).flatten()\n",
    "nr_values[labels_test[\"nrtend_TAU\"] == -1] = (-10 ** output_scalers[\"nrtend_TAU\"][-1].inverse_transform(\n",
    "    scaled_out_test.loc[labels_test[\"nrtend_TAU\"] == -1, [\"nrtend_TAU\"]])).flatten()\n",
    "unscaled_tendencies.loc[:, \"nrtend_TAU\"] = nr_values\n",
    "\n",
    "unscaled_tendencies.loc[labels_test[\"nctend_TAU\"] == 1, \"nctend_TAU\"] = (-10 ** output_scalers[\"nctend_TAU\"][1].inverse_transform(\n",
    "    scaled_out_test.loc[labels_test[\"nctend_TAU\"] == 1, [\"nctend_TAU\"]])).ravel()\n",
    "\n",
    "unscaled_tendencies.loc[labels_test[\"qrtend_TAU\"] == 1, \"qrtend_TAU\"] = (10 ** output_scalers[\"qrtend_TAU\"][1].inverse_transform(\n",
    "    scaled_out_test.loc[labels_test[\"qrtend_TAU\"] == 1, [\"qrtend_TAU\"]])).ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output visualizations\n",
    "\n",
    "f, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(16,4))\n",
    "\n",
    "output_col = \"nrtend_TAU\"\n",
    "colp = unscaled_tendencies[output_col]\n",
    "col = pred_tendencies[output_col]\n",
    "ax1.hist(np.log10(-colp[colp < 0]), label=\"<0 pred\", color='skyblue')\n",
    "ax1.hist(np.log10(colp[colp > 0]), label=\">0 pred\", color='pink')\n",
    "ax1.hist(np.log10(-col[col < 0]), label=\"<0 true\", histtype=\"step\", color=\"navy\", lw=3)\n",
    "ax1.hist(np.log10(col[col > 0]), label=\">0 true\", histtype=\"step\", color=\"purple\", lw=3)\n",
    "ax1.set_xlabel(output_col)\n",
    "ax1.set_ylabel('log10')\n",
    "ax1.title.set_text(f\"log10-transformed {output_col} output data\\nfiltered by output_transform ops\")\n",
    "ax1.set_yscale('log')\n",
    "ax1.legend()\n",
    "\n",
    "output_col = \"nctend_TAU\"\n",
    "colp = unscaled_tendencies[output_col]\n",
    "col = pred_tendencies[output_col]\n",
    "ax2.hist(np.log10(-colp[colp < 0]), label=\"pred\", color='skyblue')\n",
    "ax2.hist(np.log10(-col[col < 0]), label=\"true\", histtype=\"step\", color=\"navy\", lw=3)\n",
    "ax2.set_xlabel(output_col)\n",
    "ax2.set_ylabel('log10')\n",
    "ax2.title.set_text(f\"log10-transformed {output_col} output data\\nfiltered by output_transform ops\")\n",
    "ax2.set_yscale('log')\n",
    "\n",
    "output_col = \"qrtend_TAU\"\n",
    "colp = unscaled_tendencies[output_col]\n",
    "col = pred_tendencies[output_col]\n",
    "ax3.hist(np.log10(colp[colp > 0]), label=\"pred\", color='skyblue')\n",
    "ax3.hist(np.log10(col[col > 0]), label=\"true\", histtype=\"step\", color=\"navy\", lw=3)\n",
    "ax3.set_xlabel(output_col)\n",
    "ax3.set_ylabel('log10')\n",
    "ax3.title.set_text(f\"log10-transformed {output_col} output data\\nfiltered by output_transform ops\")\n",
    "ax3.set_yscale('log')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n",
    "\n",
    "\n",
    "Albrecht, B. A.  (1989).  Aerosols, cloud microphysics and fractional cloudiness.Sci-449ence,245, 1227–1230.\n",
    "\n",
    "Bodas-Salcedo, A., Mulcahy, J. P., Andrews, T., Williams, K. D., Ringer, M. A.,455Field, P. R., & Elsaesser, G. S.(2019).Strong Dependence of Atmospheric456Feedbacks on Mixed-Phase Microphysics and Aerosol-Cloud Interactions in457HadGEM3.Journal of Advances in Modeling Earth Systems,11(6), 1735–1758.458doi:  10.1029/2019MS001688\n",
    "\n",
    "Bogenschutz, P. A., Gettelman, A., Morrison, H., Larson, V. E., Craig, C., & Scha-460nen, D. P.(2013).Higher-order turbulence closure and its impact on Climate461Simulation in the Community Atmosphere Model.Journal of Climate,26(23),4629655–9676.  doi:  10.1175/JCLI-D-13-00075.1\n",
    "\n",
    "Danabasoglu, G., Lamarque, J.-F., Bacmeister, J., Bailey, D. A., DuVivier, A. K.,471Edwards, J., . . .  Strand, W. G.(2020).The Community Earth System Model472Version 2 (CESM2).Journal of Advances in Modeling Earth Systems,12(2),473e2019MS001916.  doi:  10.1029/2019MS001916\n",
    "\n",
    "Forbes, R. M., & Ahlgrimm, M.(2014, September).On the Representation of475High-Latitude Boundary Layer Mixed-Phase Cloud in the ECMWF Global Model.476Monthly Weather Review,142(9), 3425–3445.  doi:  10.1175/MWR-D-13-00325.1\n",
    "\n",
    "Gettelman, A.(2015, November).Putting the clouds back in aerosol–cloud inter-478actions.Atmos. Chem. Phys.,15(21), 12397–12411.doi:  10.5194/acp-15-12397479-2015480\n",
    "\n",
    "Gettelman, A., Bardeen, C. G., McCluskey, C. S., & Jarvinen, E.    (2020).    Simulat-481ing Observations of Southern Ocean Clouds and Implications for Climate.J. Adv.482Model. Earth Syst..  doi:  10.1029/2020JD032619483\n",
    "\n",
    "Gettelman, A., Hannay, C., Bacmeister, J. T., Neale, R. B., Pendergrass, A. G.,484Danabasoglu, G., . . .  Mills, M. J.(2019).High Climate Sensitivity in the Com-485munity Earth System Model Version 2 (CESM2).Geophysical Research Letters,48646(14), 8329–8337.  doi:  10.1029/2019GL083978487\n",
    "\n",
    "Gettelman, A., & Morrison, H.   (2015).   Advanced Two-Moment Bulk Microphysics488for Global Models. Part I: Off-Line Tests and Comparison with Other Schemes.J.489Climate,28(3), 1268–1287.  doi:  10.1175/JCLI-D-14-00102.1490\n",
    "\n",
    "Gettelman, A., Morrison, H., Santos, S., Bogenschutz, P., & Caldwell, P. M.   (2015).491Advanced Two-Moment Bulk Microphysics for Global Models. Part II: Global492Model Solutions and Aerosol–Cloud Interactions.J. Climate,28(3), 1288–1307.493doi:  10.1175/JCLI-D-14-00103.1494\n",
    "\n",
    "Gettelman, A., & Sherwood, S. C.  (2016, October).  Processes Responsible for Cloud495Feedback.Curr Clim Change Rep, 1–11.  doi:  10.1007/s40641-016-0052-8\n",
    "\n",
    "Golaz, J.-C., Larson, V. E., & Cotton, W. R.(2002).A PDF-Based Model for497Boundary Layer Clouds. Part II: Model Results.J. Atmos. Sci.,59, 3552–3571.\n",
    "\n",
    "Hoose, C., Kristj ́ansson, J. E., Chen, J.-P., & Hazra, A.  (2010, March).  A Classical-499Theory-Based Parameterization of Heterogeneous Ice Nucleation by Mineral Dust,500Soot, and Biological Particles in a Global Climate Model.J. Atmos. Sci.,67(8),5012483–2503.  doi:  10.1175/2010JAS3425.1\n",
    "\n",
    "Iacono, M. J., Mlawer, E. J., Clough, S. A., & Morcrette, J.-J.  (2000).  Impact of an503improved longwave radiation model, RRTM, on the energy budget and thermody-504namic properties of the NCAR community climate model, CCM3.jgr,105(D11),50514,873–14,890.\n",
    "\n",
    "Khairoutdinov, M. F., & Kogan, Y.  (2000).  A new cloud physics parameterization in507a large-eddy simulation model of marine stratocumulus.Monthly Weather Review,508128, 229–243.\n",
    "\n",
    "Larson, V. E., Golaz, J.-C., & Cotton, W. R.(2002, December).Small-Scale and510Mesoscale Variability in Cloudy Boundary Layers:  Joint Probability Density Func-511tions.J. Atmos. Sci.,59(24), 3519–3539.   doi:  10.1175/1520-0469(2002)059〈3519:512SSAMVI〉2.0.CO;2\n",
    "\n",
    "Liu, X., & Penner, J. E.  (2005).  Ice Nucleation Parameterization for Global Models.514Meteor. Z.,14(499-514).\n",
    "\n",
    "Michibata, T., & Takemura, T.(2015, September).Evaluation of autoconversion520schemes in a single model framework with satellite observations.J. Geophys. Res.521Atmos.,120(18), 2015JD023818.  doi:  10.1002/2015JD023818\n",
    "\n",
    "Neale, R. B., Richter, J. H., & Jochum, M.(2008).The Impact of Convection on523ENSO: From a Delayed Oscillator to a Series of Events.J. Climate,21, 5904-+.doi:  10.1175/2008JCLI2244.1\n",
    "\n",
    "Pruppacher, H. R., & Klett, J. D.   (1997).Microphysics of Clouds and Precipitation526(Second ed.).  Kluwer Academic.\n",
    "\n",
    "Seifert, A., & Beheng, K. D.  (2001).  A double-moment parameterization for simulat-531ing autoconversion, accretion and selfcollection.Atmos. Res.,59-60, 265–281.\n",
    "\n",
    "Shi, X., Liu, X., & Zhang, K.  (2015, February).  Effects of pre-existing ice crystals on536cirrus clouds and comparison between different ice nucleation parameterizations537with the Community Atmosphere Model (CAM5).Atmospheric Chemistry and538Physics,15(3), 1503–1520.  doi:  10.5194/acp-15-1503-2015\n",
    "\n",
    "Twomey, S.  (1977).  The influence of pollution on the shortwave albedo of clouds.J.553Atmos. Sci.,34(7), 1149–1152.\n",
    "\n",
    "Wang, Y., Liu, X., Hoose, C., & Wang, B.(2014, October).Different contact555angle distributions for heterogeneous ice nucleation in the Community Atmo-556spheric Model version 5.Atmos. Chem. Phys.,14(19), 10411–10430.doi:55710.5194/acp-14-10411-2014\n",
    "\n",
    "Zhang, G. J., & McFarlane, N. A.    (1995).    Sensitivity of climate simulations to the559parameterization of cumulus convection in the Canadian Climate Center general560circulation model.Atmos. Ocean,33, 407–446."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hackathon Challenges\n",
    "\n",
    "### Monday\n",
    "* Load the data\n",
    "* Create an exploratory visualization of the data\n",
    "* Test two different transformation and scaling methods\n",
    "* Test one dimensionality reduction method\n",
    "* Train a linear model\n",
    "* Train a decision tree ensemble method of your choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Monday's code goes here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuesday\n",
    "* Train a densely connected neural network\n",
    "* Train a convolutional or recurrent neural network (depends on problem)\n",
    "* Experiment with different architectures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tuesday's code goes here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wednesday\n",
    "* Calculate three relevant evaluation metrics for each ML solution and baseline\n",
    "* Refine machine learning approaches and test additional hyperparameter settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wednesday's code goes here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Thursday \n",
    "* Evaluate two interpretation methods for your machine learning solution\n",
    "* Compare interpretation of baseline with your approach\n",
    "* Submit best results on project to leaderboard\n",
    "* Prepare 2 Google Slides on team's approach and submit them "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Thursday's code goes here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ultimate Submission Code\n",
    "Please insert your full data processing and machine learning pipeline code in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
