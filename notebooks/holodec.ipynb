{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AI for Earth System Science Hackathon 2020\n",
    "# HOLODEC Machine Learning Challenge Problem\n",
    "Matt Hayman, Aaron Bansemer, David John Gagne, Gabrielle Gantos, Gunther Wallach, Natasha Flyer\n",
    "\n",
    "## Introduction\n",
    "<center><img src='holodec_images/image2.png'><center>\n",
    "\n",
    "The properties of the water and ice particles in clouds are critical to many aspects of weather and climate.  The size, shape, and concentration of ice particles control the radiative properties of cirrus clouds.  The spatial distribution of water droplets in warm clouds may influence the formation of drizzle and rain.  The interactions among droplets, ice particles, and aerosols impact precipitation, lightning, atmospheric chemistry, and more.  Measurements of natural cloud particles are often taken aboard research aircraft with instruments mounted on the wings.  One of the newer technologies used for these instruments is inline holographic imaging, which has the important advantage of being able to instantaneously record all of the particles inside a small volume of air.  Using this technology, the Holographic Detector for Clouds (HOLODEC) has been developed by the university community and NCAR to improve our cloud measurement capabilities.\n",
    "\n",
    "A hologram captures electro-magnatic field amplitude and phase (or wavefront) incident on a detector.  In contrast, standard imaging captures only the amplitude of the electric field.  Unlike a standard image, holograms can be computationally refocused on any object within the capture volume using standard wave propagation calculations. The figure below shows an example of an inline hologram (large image) with five out of focus particles.  The five smaller images show the reconstruction from each particle by computationally propagating the electro-magnetic field back to the depth position of each particle. \n",
    "\n",
    "<center><img src='holodec_images/image5.png'><center>\n",
    "\n",
    "HOLODEC is an airborne holographic cloud imager capable of capturing particle size distributions in a single shot, so a measured particle size distribution is localized to a specific part of the cloud (not accumulated over a long path length).  By capturing a hologram, each particle can be imaged irrespective of its location in the sample volume, and its size and position can be accurately captured.\n",
    "\n",
    "While holographic imaging provides unparalleled information about cloud particles, processing the raw holograms is also computationally expensive.  Lacking prior knowledge of the particle position in depth, a typical HOLODEC hologram is reconstructed at 1000 planes (or depths) using standard diffraction calculations.  At each plane, a particle’s image sharpness is evaluated and the particle size and position is determined only at a plane where it is in focus.  In addition to the computational cost, the processing requires human intervention to recognize when a “particle” is really just artifacts of interfering scattered fields.\n",
    "\n",
    "The objective of this project is to develop a machine learning solution to process HOLODEC data that is more computationally efficient than the first-principles based processor.  \n",
    "\n",
    "An important factor in processing hologram data is that the scattered field from a particle spreads out as it propagates.  The image below shows the scattered field from a 50 µm particle at distances in increments of 0.1 mm from the particle (0 to 0.7 mm).  As the scattered field expands, it’s power is also distributed over a larger area.\n",
    "\n",
    "![holodec 3d](holodec_images/image1.png)\n",
    "\n",
    "For simplicity, this project deals with simulated holographic data where particle shapes are limited to spheres.  Two datasets are provided.  The first dataset contains only one particle per hologram.  If you are successful in processing the first dataset, or you wish to immediately focus on a more challenging case, you can work on the second dataset that contains three particles per hologram.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Software Requirements\n",
    "This notebook requires Python >= 3.7. The following libraries are required:\n",
    "* numpy\n",
    "* scipy\n",
    "* matplotlib\n",
    "* xarray\n",
    "* pandas\n",
    "* scikit-learn\n",
    "* tensorflow >= 2.1\n",
    "* netcdf4\n",
    "* h5netcdf\n",
    "* tqdm\n",
    "* s3fs\n",
    "* zarr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install numpy scipy matplotlib xarray pandas scikit-learn tensorflow netcdf4 h5netcdf tqdm s3fs zarr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if working on google colab\n",
    "! pip install -U -q PyDrive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "The datasets consist of synthetically-generated holograms of cloud droplets.  Each dataset is in zarr format, and contains a series of hologram images as well as the properties of each particle in the image.  The zarr variable names and properties are as follows:\n",
    "\n",
    "| Variable Name | Description | Dimensions | Units/Range|\n",
    "| ------------- | :----:|:----------- |:------|\n",
    "| image  | Stack of single-color images. Each image is 600x400 pixels, ranging from 0-255 in intensity. | nHolograms, 600, 400 | 0 to 255 (grayscale image) |\n",
    "| x  |  X-position of each particle in the dataset.  The origin is at the center of the hologram image. | nParticles (can vary) | -888 to 888 micrometers |\n",
    "| y  | Y-position of each particle in the dataset.  The origin is at the center of the hologram image. |  nParticles (can vary) | -592 to 592 micrometers |\n",
    "| z  | Z-position of each particle in the dataset.  The origin is at the focal plane of the instrument (all particles are unfocused). | nParticles (can vary) | 14000 to 158000 micrometers |\n",
    "| d  | Diameter of each simulated droplet | nParticles (can vary) | 20 to 70 micrometers |\n",
    "| hid | Hologram ID specifies which hologram this particle is contained in.  For example, if hid=1, the corresponding x, y, z, and d variables are found in the first hologram. | nParticles (can vary) | 1 to nHolograms |\n",
    "| Dx (global attribute) | Resolution of each pixel, == 2.96 micrometers.  Use if you wish to convert x/y position to pixel number |  |  |\n",
    "\n",
    "There are two datasets for this project, a single-particle dataset and a three-particle dataset.  The single-particle dataset only contains one particle per hologram (nHolograms = nParticles). There are 50,000 holograms in the training dataset that correspond to 50,000 particles.\n",
    "\n",
    "The three-particle dataset contains three particles per hologram.  This dataset also contains 50,000 holograms but 150,000 particles.  Be sure to use the hid variable to figure out which hologram a particle is contained in.\n",
    "\n",
    "The ultimate goal of this project is to be able to find particles in the holograms and determine their x, y, z, and d values. This process is straightforward for finding a single particle, but finding multiple particles and their properties is much more challenging. A simpler objective that could also assist in speeding up the HOLODEC processing is calculating the relative distribution of particle mass in the z-direction from the holograms, which is a combination of information from z and d. \n",
    "\n",
    "<center><img src='holodec_images/image4.png'><center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Potential Input Variables\n",
    "| Variable Name | Units | Description | Relevance |\n",
    "| ------------- | :----:|:----------- | :--------:|\n",
    "| hologram   |  arbitrary |  8 bit (0-255) amplitude captured by CCD  | standard input data for processing  |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output Variables\n",
    "| Variable Name | Units | Description |\n",
    "| ------------- | :----:|:----------- |\n",
    "| x  |  µm     |  particle horizontal position |\n",
    "| y  |  µm     |  particle vertical position  |\n",
    "| z  | µm  | particle position in depth (along the direction of propagation) |\n",
    "| d  | µm  | particle diameter |\n",
    "| hid | arbitrary | hologram ID by particle|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Set\n",
    "\n",
    "The single-particle training dataset is in the zarr format described above, with 15,000 holograms and 15,000 corresponding particles.\n",
    "\n",
    "The three-particle training dataset contains 15,000 holograms and 45,000 particles.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation Set\n",
    "The single-particle validation dataset is in the zarr format described above, with 5,000 holograms and 5,000 corresponding particles.\n",
    "\n",
    "The three-particle validation dataset contains 5,000 holograms and 15,000 particles.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Set\n",
    "The single-particle test dataset is in the zarr format described above, with 5,000 holograms and 5,000 corresponding particles.\n",
    "\n",
    "The three-particle test dataset contains 5,000 holograms and 15,000 particles.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Transforms\n",
    "\n",
    "The input images only need to be normalized between 0 and 1 by dividing by 255. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Module imports \n",
    "import argparse\n",
    "import random\n",
    "import os\n",
    "from os.path import join, exists\n",
    "import sys\n",
    "import s3fs\n",
    "import yaml\n",
    "import zarr\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, MaxAbsScaler, RobustScaler\n",
    "from sklearn.metrics import mean_absolute_error, max_error\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Conv2D, Dense, Flatten, MaxPool2D\n",
    "from tensorflow.keras.models import Model, save_model\n",
    "from tensorflow.keras.optimizers import Adam, SGD\n",
    "\n",
    "seed = 328942\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "tf.random.set_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Limit GPU memory usage\n",
    "gpus = tf.config.get_visible_devices(\"GPU\")\n",
    "for device in gpus:\n",
    "    print(device)\n",
    "    tf.config.experimental.set_memory_growth(device, True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define some datset helper functions\n",
    "\n",
    "num_particles_dict = {\n",
    "    1 : '1particle',\n",
    "    3 : '3particle',\n",
    "    'multi': 'multiparticle'}\n",
    "\n",
    "split_dict = {\n",
    "    'train' : 'training',\n",
    "    'test'   : 'test',\n",
    "    'valid': 'validation'}\n",
    "\n",
    "def dataset_name(num_particles, split, file_extension='zarr'):\n",
    "    \"\"\"\n",
    "    Return the dataset filename given user inputs\n",
    "    \n",
    "    Args: \n",
    "        num_particles: (int or str) Number of particles per hologram (1, 3, or 'multi')\n",
    "        split: (str) Dataset split of either 'train', 'valid', or 'test'\n",
    "        file_extension: (str) Dataset file extension\n",
    "    \n",
    "    Returns:\n",
    "        dataset: (str) Dataset name\n",
    "    \"\"\"\n",
    "    \n",
    "    valid = [1,3,'multi']\n",
    "    if num_particles not in valid:\n",
    "        raise ValueError(\"results: num_particles must be one of %r.\" % valid)\n",
    "    num_particles = num_particles_dict[num_particles]\n",
    "    \n",
    "    valid = ['train','test','valid']\n",
    "    if split not in valid:\n",
    "        raise ValueError(\"results: split must be one of %r.\" % valid)\n",
    "    split = split_dict[split]\n",
    "    \n",
    "    return f'synthetic_holograms_{num_particles}_{split}_small.{file_extension}'\n",
    "\n",
    "def open_zarr(path_data, num_particles, split):\n",
    "    \"\"\"\n",
    "    Open a HOLODEC Zarr file hosted on AWS\n",
    "    \n",
    "    Args: \n",
    "        path_data: (str) Path to directory containing datset\n",
    "        num_particles: (int or str) Number of particles per hologram (1, 3, or 'multi')\n",
    "        split: (str) Dataset split of either 'train', 'valid', or 'test'\n",
    "    \n",
    "    Returns:\n",
    "        dataset: (xarray Dataset) Opened dataset\n",
    "    \"\"\"\n",
    "    path_data = os.path.join(path_data, dataset_name(num_particles, split))\n",
    "    fs = s3fs.S3FileSystem(anon=True, default_fill_cache=False)\n",
    "    store = s3fs.S3Map(root=path_data, s3=fs, check=False)\n",
    "    dataset = xr.open_zarr(store=store)\n",
    "    return dataset\n",
    "\n",
    "def scale_images(images, scaler_vals=None):\n",
    "    \"\"\"\n",
    "    Takes in array of images and scales pixel values between 0 and 1\n",
    "    \n",
    "    Args: \n",
    "        images: (np array) Array of images \n",
    "        scaler_vals: (dict) Image scaler 'max' and 'min' values\n",
    "        \n",
    "    Returns:\n",
    "        images_scaled: (np array) Scaled array of images with pixel values between 0 and 1\n",
    "        scaler_vals: (dict) Image scaler 'max' and 'min' values\n",
    "    \"\"\"\n",
    "    \n",
    "    if scaler_vals is None:\n",
    "        scaler_vals = {}\n",
    "        scaler_vals[\"min\"] = images.min()\n",
    "        scaler_vals[\"max\"] = images.max()\n",
    "    images_scaled = (images.astype(np.float32) - scaler_vals[\"min\"]) / (scaler_vals[\"max\"] - scaler_vals[\"min\"])\n",
    "    return images_scaled, scaler_vals\n",
    "\n",
    "def load_scaled_datasets(path_data, num_particles, output_cols, slice_idx,\n",
    "                         split='train', scaler_vals=None):\n",
    "    \"\"\"\n",
    "    Given a path to training or validation datset, the number of particles per\n",
    "    hologram, and output columns, returns scaled inputs and raw outputs.\n",
    "    \n",
    "    Args: \n",
    "        path_data: (str) Path to directory containing training and validation datsets\n",
    "        num_particles: (int or str) Number of particles per hologram (1, 3, or 'multi') \n",
    "        output_cols: (list of strings) List of feature columns to be used\n",
    "        \n",
    "    Returns:\n",
    "        inputs_scaled: (np array) Input data scaled between 0 and 1\n",
    "        outputs: (df) Output data specified by output_cols\n",
    "        scaler_vals: (dict) list of training/validation/test files\n",
    "    \"\"\"\n",
    "    \n",
    "    if split == 'valid':\n",
    "      slice_idx = int(slice_idx/3)\n",
    "    print(\"Slicing data into inputs/outputs\")\n",
    "    ds = open_zarr(path_data, num_particles, split)\n",
    "    inputs = ds[\"image\"].values[:slice_idx]\n",
    "    outputs = ds[output_cols].to_dataframe().loc[:slice_idx-1,:]\n",
    "    ds.close()\n",
    "    print(f\"\\t- outputs.shape: {outputs.shape}\")\n",
    "\n",
    "    print(\"Scaling input data\")\n",
    "    if split == 'train':\n",
    "      inputs_scaled, scaler_vals = scale_images(inputs)\n",
    "    else:\n",
    "      slice_idx = int(slice_idx/3)\n",
    "      inputs_scaled, _ = scale_images(inputs, scaler_vals)\n",
    "  \n",
    "    inputs_scaled = np.expand_dims(inputs_scaled, -1)\n",
    "    print(f\"\\t- inputs_scaled.shape: {inputs_scaled.shape}\")\n",
    "\n",
    "    return inputs_scaled, outputs, scaler_vals\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data definitions\n",
    "\n",
    "path_data = \"ncar-aiml-data-commons/holodec/\"\n",
    "num_particles = 3\n",
    "output_cols = [\"hid\", \"x\", \"y\", \"z\", \"d\"]\n",
    "num_z_bins = 20\n",
    "slice_idx = 6000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load and normalize data (this takes approximately 2 minutes)\n",
    "train_inputs_scaled,\\\n",
    "train_outputs,\\\n",
    "scaler_vals = load_scaled_datasets(path_data,\n",
    "                                   num_particles,\n",
    "                                   output_cols,\n",
    "                                   slice_idx)\n",
    "\n",
    "valid_inputs_scaled,\\\n",
    "valid_outputs, _ = load_scaled_datasets(path_data,\n",
    "                                        num_particles,\n",
    "                                        output_cols,\n",
    "                                        slice_idx,\n",
    "                                        split='valid',\n",
    "                                        scaler_vals=scaler_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot a single hologram with the particles overlaid\n",
    "def plot_hologram(h, outputs):\n",
    "    \"\"\"\n",
    "    Given a hologram number, plot hologram and particle point\n",
    "    \n",
    "    Args: \n",
    "        h: (int) hologram number\n",
    "    \n",
    "    Returns:\n",
    "        print of pseudocolor plot of hologram and hologram particles\n",
    "    \"\"\"    \n",
    "    x_vals = np.linspace(-888, 888, train_inputs_scaled[h, :, :, 0].shape[0])\n",
    "    y_vals = np.linspace(-592, 592, train_inputs_scaled[h, :, :, 0].shape[1])\n",
    "\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.pcolormesh(x_vals, y_vals, train_inputs_scaled[h, :, :, 0].T, cmap=\"RdBu_r\")\n",
    "    h_particles = np.where(outputs[\"hid\"] == h + 1)[0]\n",
    "    for h_particle in h_particles:\n",
    "        plt.scatter(outputs.loc[h_particle, \"x\"],\n",
    "                    outputs.loc[h_particle, \"y\"],\n",
    "                    outputs.loc[h_particle, \"d\"] ** 2,\n",
    "                    outputs.loc[h_particle, \"z\"],\n",
    "                    vmin=outputs[\"z\"].min(),\n",
    "                    vmax=outputs[\"z\"].max(),\n",
    "                    cmap=\"cool\")\n",
    "        plt.annotate(f\"d: {outputs.loc[h_particle,'d']:.1f} µm\",\n",
    "                     (outputs.loc[h_particle, \"x\"], outputs.loc[h_particle, \"y\"]))\n",
    "    plt.xlabel(\"horizontal particle position (µm)\", fontsize=16)\n",
    "    plt.ylabel(\"vertical particle position (µm)\", fontsize=16)\n",
    "    plt.title(\"Hologram and particle positions plotted in four dimensions\", fontsize=20, pad=20)\n",
    "    plt.colorbar().set_label(label=\"z-axis particle position (µm)\", size=16)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = 300\n",
    "plot_hologram(h, train_outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline Machine Learning Model\n",
    "A baseline model for solving this problem uses a ConvNET architecture implemented in Keras.  The first three convolution layers consist of 5 x 5 pixel kernels with rectified linear unit (relu) activation followed by a 4 x 4 pixel max pool layer.  The first convolution layer has 8 channels, the second contains 16 channels, and the third contains 32 channels.  The output of the third convolution layer is flattened and fed into a dense layer with 64 neurons and relu activation.  Finally the output layer consists of the relative mass in 20 bins.  The model is trained using a mean absolute error and categorical cross-entropy loss function.\n",
    "\n",
    "Training time: 20 epochs in ~2.5 minutes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv2DNeuralNetwork(object):\n",
    "    \"\"\"\n",
    "    A Conv2D Neural Network Model that can support arbitrary numbers of layers.\n",
    "\n",
    "    Attributes:\n",
    "        filters: List of number of filters in each Conv2D layer\n",
    "        kernel_sizes: List of kernel sizes in each Conv2D layer\n",
    "        conv2d_activation: Type of activation function for conv2d layers\n",
    "        pool_sizes: List of Max Pool sizes\n",
    "        dense_sizes: Sizes of dense layers\n",
    "        dense_activation: Type of activation function for dense layers\n",
    "        output_activation: Type of activation function for output layer\n",
    "        lr: Optimizer learning rate\n",
    "        optimizer: Name of optimizer or optimizer object.\n",
    "        adam_beta_1: Exponential decay rate for the first moment estimates\n",
    "        adam_beta_2: Exponential decay rate for the first moment estimates\n",
    "        sgd_momentum: Stochastic Gradient Descent momentum\n",
    "        decay: Optimizer decay\n",
    "        loss: Name of loss function or loss object\n",
    "        batch_size: Number of examples per batch\n",
    "        epochs: Number of epochs to train\n",
    "        verbose: Level of detail to provide during training\n",
    "        model: Keras Model object\n",
    "    \"\"\"\n",
    "    def __init__(self, filters=(8,), kernel_sizes=(5,), conv2d_activation=\"relu\",\n",
    "                 pool_sizes=(4,), dense_sizes=(64,), dense_activation=\"relu\", output_activation=\"softmax\",\n",
    "                 lr=0.001, optimizer=\"adam\",  adam_beta_1=0.9, adam_beta_2=0.999,\n",
    "                 sgd_momentum=0.9, decay=0, loss=\"mse\", batch_size=32, epochs=2, verbose=0):\n",
    "        self.filters = filters\n",
    "        self.kernel_sizes = [tuple((v,v)) for v in kernel_sizes]\n",
    "        self.conv2d_activation = conv2d_activation\n",
    "        self.pool_sizes = [tuple((v,v)) for v in pool_sizes]\n",
    "        self.dense_sizes = dense_sizes\n",
    "        self.dense_activation = dense_activation\n",
    "        self.output_activation = output_activation\n",
    "        self.lr = lr\n",
    "        self.optimizer = optimizer\n",
    "        self.optimizer_obj = None\n",
    "        self.adam_beta_1 = adam_beta_1\n",
    "        self.adam_beta_2 = adam_beta_2\n",
    "        self.sgd_momentum = sgd_momentum\n",
    "        self.decay = decay\n",
    "        self.loss = loss\n",
    "        self.batch_size = batch_size\n",
    "        self.epochs = epochs\n",
    "        self.verbose = verbose\n",
    "        self.model = None\n",
    "\n",
    "    def build_neural_network(self, input_shape, output_shape):\n",
    "        \"\"\"Create Keras neural network model and compile it.\"\"\"\n",
    "        conv_input = Input(shape=(input_shape), name=\"input\")\n",
    "        nn_model = conv_input\n",
    "        for h in range(len(self.filters)):\n",
    "            nn_model = Conv2D(self.filters[h], self.kernel_sizes[h], padding=\"same\",\n",
    "                              activation=self.conv2d_activation, name=f\"conv2D_{h:02d}\")(nn_model)\n",
    "            nn_model = MaxPool2D(self.pool_sizes[h], name=f\"maxpool2D_{h:02d}\")(nn_model)\n",
    "        nn_model = Flatten()(nn_model)\n",
    "        for h in range(len(self.dense_sizes)):\n",
    "            nn_model = Dense(self.dense_sizes[h], activation=self.dense_activation, name=f\"dense_{h:02d}\")(nn_model)\n",
    "        nn_model = Dense(output_shape, activation=self.output_activation, name=f\"dense_output\")(nn_model)\n",
    "        self.model = Model(conv_input, nn_model)\n",
    "        if self.optimizer == \"adam\":\n",
    "            self.optimizer_obj = Adam(lr=self.lr, beta_1=self.adam_beta_1, beta_2=self.adam_beta_2, decay=self.decay)\n",
    "        elif self.optimizer == \"sgd\":\n",
    "            self.optimizer_obj = SGD(lr=self.lr, momentum=self.sgd_momentum, decay=self.decay)\n",
    "        self.model.compile(optimizer=self.optimizer, loss=self.loss)\n",
    "        self.model.summary()\n",
    "\n",
    "    def fit(self, x, y, xv, yv):\n",
    "        if len(y.shape) == 1:\n",
    "            output_shape = 1\n",
    "        else:\n",
    "            output_shape = y.shape[1]\n",
    "        input_shape = x.shape[1:]\n",
    "        self.build_neural_network(input_shape, output_shape)\n",
    "        self.model.fit(x, y, batch_size=self.batch_size, epochs=self.epochs,\n",
    "                       verbose=self.verbose, validation_data=(xv, yv))\n",
    "        return self.model.history.history\n",
    "\n",
    "    def predict(self, x):\n",
    "        y_out = self.model.predict(x, batch_size=self.batch_size)\n",
    "        return y_out\n",
    "\n",
    "    def predict_proba(self, x):\n",
    "        y_prob = self.model.predict(x, batch_size=self.batch_size)\n",
    "        return y_prob\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Z Relative Particle Mass Model\n",
    "This neural network is tasked to predict the distribution of particle mass in the z-plane of the instrument. The relative mass is calculated by calculating the volume of each sphere based on the area and dividing by the total mass of all particles. The advantage of this target is that it behaves like a probability density function and sums to 1, and it is agnostic to the number of particles in the image.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_z_relative_mass(outputs, holograms, num_z_bins=20, z_bins=None):\n",
    "    \"\"\"\n",
    "    Calculate z-relative mass from particle data.\n",
    "    \n",
    "    Args: \n",
    "        outputs: (np array) Output data previously specified by output_cols \n",
    "        holograms: (int) Number of holograms\n",
    "        num_z_bins: (int) Number of bins for z_bins linspace\n",
    "        z_bins: (np array) Bin linspace along the z-axis\n",
    "    \n",
    "    Returns:\n",
    "        z_mass: (np array) Particle mass distribution by hologram\n",
    "        z_bins: (np array) Bin linspace along the z-axis\n",
    "    \"\"\"\n",
    "    \n",
    "    if z_bins is None:\n",
    "        z_bins = np.linspace(outputs[\"z\"].min()- 100, outputs[\"z\"].max() + 100, num_z_bins)\n",
    "        print(z_bins)\n",
    "    else:\n",
    "        num_z_bins = z_bins.size\n",
    "    z_mass = np.zeros((holograms, num_z_bins), dtype=np.float32)\n",
    "    for i in range(outputs.shape[0]):\n",
    "        z_pos = np.searchsorted(z_bins, outputs.loc[i, \"z\"], side=\"right\") - 1\n",
    "        mass = 4 / 3 * np.pi * (outputs.loc[i, \"d\"])**3\n",
    "        z_mass[int(outputs.loc[i, \"hid\"]) - 1, z_pos] += mass\n",
    "    z_mass /= np.expand_dims(z_mass.sum(axis=1), -1)\n",
    "    print(f\"z_mass.shape: {z_mass.shape}\\nz_bins.shape: {z_bins.shape}\")\n",
    "    return z_mass, z_bins\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_bins = np.linspace(np.minimum(train_outputs[\"z\"].min(), valid_outputs[\"z\"].min()),\n",
    "                     np.maximum(train_outputs[\"z\"].max(), valid_outputs[\"z\"].max()),\n",
    "                     num_z_bins)\n",
    "\n",
    "train_z_mass, _ = calc_z_relative_mass(train_outputs, train_inputs_scaled.shape[0], z_bins=z_bins)\n",
    "valid_z_mass, _ = calc_z_relative_mass(valid_outputs, valid_inputs_scaled.shape[0], z_bins=z_bins)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Three particle, z-mass model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conv2d_network definitions for 3 particle z mass solution\n",
    "\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "if IN_COLAB:\n",
    "    path_out = \"/content/gdrive/My Drive/micro_models/3particle_base\"\n",
    "else:\n",
    "    path_out = \"./holodec_models/3particle_base/\"\n",
    "if not exists(path_out):\n",
    "    os.makedirs(path_out)\n",
    "model_name = \"cnn\"\n",
    "filters = [16, 24, 32]\n",
    "kernel_sizes = [5, 5, 5]\n",
    "conv2d_activation = \"relu\"\n",
    "pool_sizes = [4, 4, 4]\n",
    "dense_sizes = [64, 32]\n",
    "dense_activation = \"elu\"\n",
    "lr = 0.0003\n",
    "decay = 0.1\n",
    "optimizer = \"adam\"\n",
    "loss = \"categorical_crossentropy\"\n",
    "batch_size = 128\n",
    "epochs = 40\n",
    "verbose = 1\n",
    "\n",
    "seed = 328942\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "tf.random.set_seed(seed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3 particle z mass model build, compile, fit, and predict\n",
    "\n",
    "three_start = datetime.now()\n",
    "with tf.device('/device:GPU:0'):\n",
    "    mod = Conv2DNeuralNetwork(filters=filters, kernel_sizes=kernel_sizes,\n",
    "                              conv2d_activation=conv2d_activation,\n",
    "                              pool_sizes=pool_sizes, dense_sizes=dense_sizes,\n",
    "                              dense_activation=dense_activation, lr=lr,\n",
    "                              optimizer=optimizer, decay=decay, loss=loss,\n",
    "                              batch_size=batch_size, epochs=epochs, verbose=verbose)\n",
    "    hist = mod.fit(train_inputs_scaled, train_z_mass, valid_inputs_scaled, valid_z_mass)\n",
    "    \n",
    "    train_z_mass_pred = mod.predict(train_inputs_scaled)\n",
    "    valid_z_mass_pred = mod.predict(valid_inputs_scaled)\n",
    "print(f\"Running model took {datetime.now() - three_start} time\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize loss history\n",
    "\n",
    "plt.plot(hist['loss'])\n",
    "plt.plot(hist['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['training', 'validation'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model\n",
    "print(\"Saving the model\")\n",
    "mod.model.save(join(path_out, model_name +\".h5\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clear your tf session without needing to re-load and re-scale data\n",
    "\n",
    "del mod\n",
    "tf.keras.backend.clear_session()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Three Particle Metrics\n",
    "\n",
    "How well do individual predictions (red) match with the actual particle locations (blue)?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_index = 11\n",
    "bin_size = z_bins[1] - z_bins[0]\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(z_bins / 1000, valid_z_mass_pred[valid_index], bin_size / 1000, color='red', label=\"Predicted\")\n",
    "plt.bar(z_bins / 1000, valid_z_mass[valid_index], bin_size / 1000, edgecolor='blue', facecolor=\"none\", lw=3, label=\"True\")\n",
    "plt.ylim(0, 1)\n",
    "plt.xlabel(\"z-axis particle position (mm)\", fontsize=16)\n",
    "plt.ylabel(\"relative mass\", fontsize=16)\n",
    "plt.legend(loc=\"best\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the model was completely unbiased, then mean relative mass in each bin should be nearly the same across all validation examples. In this case we see that the CNN preferentially predicts that the mass is closer to the camera, likely due to a combination of particles closer to the camera blocking those farther away along with more distant particles influencing the entire image. Since the CNN assumes image properties are more localized, it will struggle to detect the particles that are farther away."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.bar(z_bins / 1000, valid_z_mass_pred.mean(axis=0), (z_bins[1] - z_bins[0]) / 1000, color='red')\n",
    "plt.bar(z_bins / 1000, valid_z_mass.mean(axis=0), (z_bins[1]-z_bins[0]) / 1000, edgecolor='blue', facecolor=\"none\", lw=3)\n",
    "plt.xlabel(\"z location (mm)\", fontsize=16)\n",
    "plt.ylabel(\"Mean Relative Mass\", fontsize=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ranked_probability_score(y_true, y_pred):\n",
    "    return np.mean((np.cumsum(y_true, axis=1) - np.cumsum(y_pred, axis=1)) ** 2) / (y_true.shape[1] -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rps_nn = ranked_probability_score(valid_z_mass, valid_z_mass_pred)\n",
    "rps_climo = ranked_probability_score(valid_z_mass, np.ones(valid_z_mass_pred.shape) / valid_z_mass_pred.shape[1])\n",
    "print(rps_nn, rps_climo)\n",
    "rpss = 1 - rps_nn / rps_climo\n",
    "print(f\"RPSS: {rpss:0.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One Particle Model\n",
    "An easier problem is predicting the location and properties of synthetic single particles.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data definitions\n",
    "\n",
    "path_data = \"ncar-aiml-data-commons/holodec/\"\n",
    "num_particles = 1\n",
    "output_cols_one  = [\"x\", \"y\", \"z\", \"d\"]\n",
    "scaler_one = MinMaxScaler()\n",
    "slice_idx = 6000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load and normalize data (this takes approximately 2 minutes)\n",
    "train_inputs_scaled_one,\\\n",
    "train_outputs_one,\\\n",
    "scaler_vals_one = load_scaled_datasets(path_data,\n",
    "                                   num_particles,\n",
    "                                   output_cols,\n",
    "                                   slice_idx)\n",
    "\n",
    "valid_inputs_scaled_one,\\\n",
    "valid_outputs_one, _ = load_scaled_datasets(path_data,\n",
    "                                        num_particles,\n",
    "                                        output_cols,\n",
    "                                        slice_idx,\n",
    "                                        split='valid',\n",
    "                                        scaler_vals=scaler_vals)\n",
    "\n",
    "# extra transform step for output_cols_one in lieu of z mass\n",
    "\n",
    "train_outputs_scaled_one = scaler_one.fit_transform(train_outputs_one[output_cols_one])\n",
    "valid_outputs_scaled_one = scaler_one.transform(valid_outputs_one[output_cols_one])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conv2d_network definitions for 1 particle 4D solution\n",
    "\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "if IN_COLAB:\n",
    "    path_out = \"/content/gdrive/My Drive/micro_models/1particle_base\"\n",
    "else:\n",
    "    path_out = \"./holodec_models/1particle_base/\"\n",
    "if not exists(path_out):\n",
    "    os.makedirs(path_out)\n",
    "model_name = \"cnn\"\n",
    "filters = [16, 24, 32]\n",
    "kernel_sizes = [5, 5, 5]\n",
    "conv2d_activation = \"relu\"\n",
    "pool_sizes = [4, 4, 4]\n",
    "dense_sizes = [64, 32]\n",
    "dense_activation = \"relu\"\n",
    "lr = 0.0001\n",
    "optimizer = \"adam\"\n",
    "loss = \"mae\"\n",
    "batch_size = 128\n",
    "epochs = 20\n",
    "verbose = 1\n",
    "\n",
    "if not exists(path_out):\n",
    "    os.makedirs(path_out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 particle 4D model build, compile, fit, and predict\n",
    "\n",
    "one_start = datetime.now()\n",
    "with tf.device('/device:GPU:0'):\n",
    "    mod = Conv2DNeuralNetwork(filters=filters, kernel_sizes=kernel_sizes, conv2d_activation=conv2d_activation,\n",
    "                     pool_sizes=pool_sizes, dense_sizes=dense_sizes, dense_activation=dense_activation,\n",
    "                     lr=lr, optimizer=optimizer, loss=loss, batch_size=batch_size, epochs=epochs, verbose=verbose)\n",
    "    mod.fit(train_inputs_scaled_one, train_outputs_scaled_one, valid_inputs_scaled_one, valid_outputs_scaled_one)\n",
    "    \n",
    "    train_preds_scaled_one = pd.DataFrame(mod.predict(train_inputs_scaled_one), columns=output_cols_one)\n",
    "    valid_preds_scaled_one = pd.DataFrame(mod.predict(valid_inputs_scaled_one), columns=output_cols_one)\n",
    "print(f\"Running model took {datetime.now() - one_start} time\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inverse transform of scaled predictions\n",
    "\n",
    "train_preds_one = pd.DataFrame(scaler_one.inverse_transform(train_preds_scaled_one.values), columns=output_cols_one)\n",
    "valid_preds_one = pd.DataFrame(scaler_one.inverse_transform(valid_preds_scaled_one.values), columns=output_cols_one)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One Particle Metrics\n",
    "An ideal solution to HOLODEC processing would leverage all the advantages of the instrument (unparalleled particle position and size accuracy) but reduce the drawbacks (processing time).  For this reason, the major components of the model assessment should include:\n",
    "\n",
    "Mean absolute error in predictions for single-particle dataset:\n",
    "\n",
    "| Variable Name | Error |\n",
    "| ------------- |:----------- |\n",
    "| x  |  20 µm     |\n",
    "| y  |  12 µm     |\n",
    "| z  |  2519 µm     |\n",
    "| d  |  1 µm     |\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate error by output_cols_one\n",
    "\n",
    "valid_maes_one = np.zeros(len(output_cols_one))\n",
    "max_errors_one = np.zeros(len(output_cols_one))\n",
    "for o, output_col in enumerate(output_cols_one):\n",
    "    valid_maes_one[o] = mean_absolute_error(valid_outputs_one[output_col], valid_preds_one[output_col])\n",
    "    max_errors_one[o] = max_error(valid_outputs_one[output_col], valid_preds_one[output_col])\n",
    "\n",
    "    print(f\"{output_col} MAE: {valid_maes_one[o]:,.0f} µm \\t\\t Max Error: {max_errors_one[o]:,.0f} µm\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hackathon Challenges\n",
    "\n",
    "### Monday\n",
    "* Load the data\n",
    "* Create an exploratory visualization of the data\n",
    "* Test two different transformation and scaling methods\n",
    "* Test one dimensionality reduction method\n",
    "* Train a linear model\n",
    "* Train a decision tree ensemble method of your choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Monday's code goes here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuesday\n",
    "* Train a densely connected neural network\n",
    "* Train a convolutional or recurrent neural network (depends on problem)\n",
    "* Experiment with different architectures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tuesday's code goes here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wednesday\n",
    "* Calculate three relevant evaluation metrics for each ML solution and baseline\n",
    "* Refine machine learning approaches and test additional hyperparameter settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wednesday's code goes here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Thursday \n",
    "* Evaluate two interpretation methods for your machine learning solution\n",
    "* Compare interpretation of baseline with your approach\n",
    "* Submit best results on project to leaderboard\n",
    "* Prepare 2 Google Slides on team's approach and submit them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Thursday's code goes here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ultimate Submission Code\n",
    "Please insert your full data processing and machine learning pipeline code in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
